{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5337,
          "sourceType": "datasetVersion",
          "datasetId": 3258
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "handgesturess",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dWanderingSoul/Image-Video-hand-gestures-HAND-GESTURE-RECOGNITION-MODEL/blob/main/handgesturess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "datamunge_sign_language_mnist_path = kagglehub.dataset_download('datamunge/sign-language-mnist')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "R8WnqravCfBF"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-11-02T12:01:20.404162Z",
          "iopub.execute_input": "2024-11-02T12:01:20.404685Z",
          "iopub.status.idle": "2024-11-02T12:01:21.784096Z",
          "shell.execute_reply.started": "2024-11-02T12:01:20.404639Z",
          "shell.execute_reply": "2024-11-02T12:01:21.782653Z"
        },
        "trusted": true,
        "id": "gtUNIkmNCfBQ",
        "outputId": "f4286424-9374-43d3-e2ca-2d91ae7957d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/sign-language-mnist/sign_mnist_test.csv\n/kaggle/input/sign-language-mnist/amer_sign2.png\n/kaggle/input/sign-language-mnist/amer_sign3.png\n/kaggle/input/sign-language-mnist/sign_mnist_train.csv\n/kaggle/input/sign-language-mnist/american_sign_language.PNG\n/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\n/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\n",
        "\n",
        "# Separate images and labels\n",
        "X_train = train_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1) / 255.0  # Normalize pixel values\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "\n",
        "X_test = test_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1) / 255.0\n",
        "y_test = test_data.iloc[:, 0].values\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=24)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:02:37.853289Z",
          "iopub.execute_input": "2024-11-02T12:02:37.853976Z",
          "iopub.status.idle": "2024-11-02T12:03:00.420436Z",
          "shell.execute_reply.started": "2024-11-02T12:02:37.85393Z",
          "shell.execute_reply": "2024-11-02T12:03:00.418373Z"
        },
        "trusted": true,
        "id": "HCF8PXPhCfBV",
        "outputId": "44e49d8b-49b6-4b18-d56d-056373193e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# One-hot encode labels\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m y_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_test, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/numerical_utils.py:89\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, num_classes))\n\u001b[0;32m---> 89\u001b[0m \u001b[43mcategorical\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     90\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[1;32m     91\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
            "\u001b[0;31mIndexError\u001b[0m: index 24 is out of bounds for axis 1 with size 24"
          ],
          "ename": "IndexError",
          "evalue": "index 24 is out of bounds for axis 1 with size 24",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for unique values in the labels\n",
        "print(\"Unique values in y_train:\", np.unique(y_train))\n",
        "print(\"Unique values in y_test:\", np.unique(y_test))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:04:38.976877Z",
          "iopub.execute_input": "2024-11-02T12:04:38.97749Z",
          "iopub.status.idle": "2024-11-02T12:04:38.989639Z",
          "shell.execute_reply.started": "2024-11-02T12:04:38.977425Z",
          "shell.execute_reply": "2024-11-02T12:04:38.988085Z"
        },
        "trusted": true,
        "id": "ogpwGaxzCfBX",
        "outputId": "64a17b2d-6704-4f60-c0a3-d33a46e01d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Unique values in y_train: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\nUnique values in y_test: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remap the labels to make them contiguous\n",
        "y_train = np.where(y_train > 8, y_train - 1, y_train)\n",
        "y_test = np.where(y_test > 8, y_test - 1, y_test)\n",
        "\n",
        "# Verify unique values after remapping\n",
        "print(\"Unique values in y_train after remapping:\", np.unique(y_train))\n",
        "print(\"Unique values in y_test after remapping:\", np.unique(y_test))\n",
        "\n",
        "# One-hot encode labels with num_classes=24\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=24)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:04:44.773241Z",
          "iopub.execute_input": "2024-11-02T12:04:44.774109Z",
          "iopub.status.idle": "2024-11-02T12:04:44.79259Z",
          "shell.execute_reply.started": "2024-11-02T12:04:44.774049Z",
          "shell.execute_reply": "2024-11-02T12:04:44.791038Z"
        },
        "trusted": true,
        "id": "1RGX-O0HCfBY",
        "outputId": "7fa8cd03-33ea-4f99-9afc-e28ad755a63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Unique values in y_train after remapping: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\nUnique values in y_test after remapping: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(24, activation='softmax')  # 24 classes for A-Y (excluding J)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:04:50.118951Z",
          "iopub.execute_input": "2024-11-02T12:04:50.119544Z",
          "iopub.status.idle": "2024-11-02T12:07:37.803692Z",
          "shell.execute_reply.started": "2024-11-02T12:04:50.119494Z",
          "shell.execute_reply": "2024-11-02T12:07:37.802381Z"
        },
        "trusted": true,
        "id": "h3GQBpSiCfBZ",
        "outputId": "12b2ae96-d208-49d9-a86e-54d133b32882"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.5033 - loss: 1.7085 - val_accuracy: 0.8699 - val_loss: 0.3961\nEpoch 2/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9891 - loss: 0.0588 - val_accuracy: 0.9052 - val_loss: 0.3420\nEpoch 3/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0119 - val_accuracy: 0.9045 - val_loss: 0.2996\nEpoch 4/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9110 - val_loss: 0.3683\nEpoch 5/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 8.0642e-04 - val_accuracy: 0.9241 - val_loss: 0.3132\nEpoch 6/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 3.9849e-04 - val_accuracy: 0.9229 - val_loss: 0.3336\nEpoch 7/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 2.3871e-04 - val_accuracy: 0.9195 - val_loss: 0.3705\nEpoch 8/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 0.8950 - val_loss: 0.4044\nEpoch 9/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9993 - loss: 0.0050 - val_accuracy: 0.9251 - val_loss: 0.3271\nEpoch 10/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 2.8516e-04 - val_accuracy: 0.9262 - val_loss: 0.3151\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: what does this mean Epoch 1/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 19s 20ms/step - accuracy: 0.5033 - loss: 1.7085 - val_accuracy: 0.8699 - val_loss: 0.3961\n",
        "# Epoch 2/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 17s 20ms/step - accuracy: 0.9891 - loss: 0.0588 - val_accuracy: 0.9052 - val_loss: 0.3420\n",
        "# Epoch 3/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 16s 19ms/step - accuracy: 0.9982 - loss: 0.0119 - val_accuracy: 0.9045 - val_loss: 0.2996\n",
        "# Epoch 4/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 17s 19ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9110 - val_loss: 0.3683\n",
        "# Epoch 5/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 16s 19ms/step - accuracy: 1.0000 - loss: 8.0642e-04 - val_accuracy: 0.9241 - val_loss: 0.3132\n",
        "# Epoch 6/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 17s 19ms/step - accuracy: 1.0000 - loss: 3.9849e-04 - val_accuracy: 0.9229 - val_loss: 0.3336\n",
        "# Epoch 7/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 16s 19ms/step - accuracy: 1.0000 - loss: 2.3871e-04 - val_accuracy: 0.9195 - val_loss: 0.3705\n",
        "# Epoch 8/10\n",
        "# 858/858 ━━━━━━━━━━━━━━━━━━━━ 17s 19ms/step - ac\n",
        "\n",
        "Okay, let's break down what the output you provided means in the context of training a neural network (specifically, a Convolutional Neural Network or CNN).\n",
        "\n",
        "**Epochs**\n",
        "\n",
        "*   **Epoch 1/10, Epoch 2/10, etc.:**  An epoch represents one complete pass through the entire training dataset. You're training your model for a total of 10 epochs. So, the output shows the progress of your model training as it goes through each epoch.\n",
        "\n",
        "**858/858 ━━━━━━━━━━━━━━━━━━━━ 19s 20ms/step**\n",
        "\n",
        "*   **858/858:** This indicates that the training process is iterating through batches of data. There are 858 batches in your training dataset, and the output shows that all batches have been processed for the current epoch.\n",
        "*   **━━━━━━━━━━━━━━━━━━━━:**  A progress bar representing the completion of the epoch.\n",
        "*   **19s:** The time it took to complete the current epoch (approximately 19 seconds).\n",
        "*   **20ms/step:** The average time taken to process each batch (approximately 20 milliseconds).\n",
        "\n",
        "\n",
        "**Metrics:**\n",
        "\n",
        "*   **accuracy: 0.5033:** This is the training accuracy for the current epoch. It measures how well the model is performing on the training data. In Epoch 1, the accuracy is 0.5033 or 50.33%, which is not very good, but it gets better in subsequent epochs.\n",
        "*   **loss: 1.7085:** This is the training loss for the current epoch. It represents the error the model is making during training. Lower loss values indicate better performance.\n",
        "*   **val_accuracy: 0.8699:** This is the validation accuracy for the current epoch. It measures how well the model is performing on a separate dataset called the validation set. This is a crucial metric as it gives an indication of how well your model will generalize to unseen data.\n",
        "*   **val_loss: 0.3961:** This is the validation loss for the current epoch. It represents the error the model is making on the validation set.\n",
        "\n",
        "\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "The output shows the model's performance during each epoch, including:\n",
        "\n",
        "*   Training accuracy and loss\n",
        "*   Validation accuracy and loss\n",
        "*   The progress of the training process\n",
        "\n",
        "**Observations**\n",
        "\n",
        "*   The model starts with relatively low accuracy (0.5033) and high loss (1.7085) in the first epoch.\n",
        "*   As training progresses, the accuracy increases significantly (approaching 1.0000), and the loss decreases (approaching 0.0000).\n",
        "*   Validation accuracy generally increases, which is a good sign. However, validation loss can fluctuate slightly, indicating some overfitting might be happening (the model is becoming too specialized to the training data and not performing as well on unseen data).\n",
        "\n",
        "\n",
        "**In Conclusion**\n",
        "\n",
        "This output gives valuable information about the performance of your CNN during training. It's essential to monitor these metrics to ensure the model is learning effectively and to avoid overfitting."
      ],
      "metadata": {
        "id": "9V8ijFxAEQMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have X_train (features) and y_train (labels) as your main dataset.\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:07:07.615666Z",
          "iopub.execute_input": "2024-11-01T21:07:07.61615Z",
          "iopub.status.idle": "2024-11-01T21:07:08.441191Z",
          "shell.execute_reply.started": "2024-11-01T21:07:07.616106Z",
          "shell.execute_reply": "2024-11-01T21:07:08.440232Z"
        },
        "trusted": true,
        "id": "Vts4lKrRCfBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load training and validation data\n",
        "train_data = train_datagen.flow_from_directory('path_to_train_data', target_size=(28, 28), color_mode='grayscale', batch_size=32, class_mode='categorical')\n",
        "val_data = val_datagen.flow_from_directory('path_to_val_data', target_size=(28, 28), color_mode='grayscale', batch_size=32, class_mode='categorical')\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout for regularization\n",
        "    Dense(24, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)\n",
        "\n",
        "# Train model with callbacks\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=20,\n",
        "    validation_data=val_data,\n",
        "    callbacks=[early_stopping, lr_reduction]\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(val_data)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}, Validation Loss: {loss:.4f}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:07:30.191718Z",
          "iopub.execute_input": "2024-11-01T21:07:30.192619Z",
          "iopub.status.idle": "2024-11-01T21:07:30.298701Z",
          "shell.execute_reply.started": "2024-11-01T21:07:30.192564Z",
          "shell.execute_reply": "2024-11-01T21:07:30.297147Z"
        },
        "trusted": true,
        "id": "4T75yKxiCfBd",
        "outputId": "f78bebff-a9c3-4451-d663-00a52a925139"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m val_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load training and validation data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_train_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m val_data \u001b[38;5;241m=\u001b[39m val_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_val_data\u001b[39m\u001b[38;5;124m'\u001b[39m, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_train_data'"
          ],
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_train_data'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:15:11.758523Z",
          "iopub.execute_input": "2024-11-01T21:15:11.75894Z",
          "iopub.status.idle": "2024-11-01T21:15:11.76436Z",
          "shell.execute_reply.started": "2024-11-01T21:15:11.7589Z",
          "shell.execute_reply": "2024-11-01T21:15:11.763157Z"
        },
        "trusted": true,
        "id": "iKBypS7oCfBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Load images and labels from arrays\n",
        "# X should be the images and y should be the corresponding labels\n",
        "# Assuming X is your image data and y is your labels array\n",
        "X = np.load('path_to_images.npy')  # Load your image data\n",
        "y = np.load('path_to_labels.npy')  # Load your labels\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:15:35.563699Z",
          "iopub.execute_input": "2024-11-01T21:15:35.564121Z",
          "iopub.status.idle": "2024-11-01T21:15:35.613961Z",
          "shell.execute_reply.started": "2024-11-01T21:15:35.564067Z",
          "shell.execute_reply": "2024-11-01T21:15:35.612449Z"
        },
        "trusted": true,
        "id": "lOMW03V3CfBi",
        "outputId": "cfc01511-6075-4529-9432-9e61c5dc3954"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Load images and labels from arrays\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# X should be the images and y should be the corresponding labels\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming X is your image data and y is your labels array\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_images.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load your image data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load your labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Split into training and validation sets\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_images.npy'"
          ],
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_images.npy'",
          "output_type": "error"
        }
      ]
    }
  ]
}