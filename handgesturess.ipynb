{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5337,
          "sourceType": "datasetVersion",
          "datasetId": 3258
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "handgesturess",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dWanderingSoul/Image-Video-hand-gestures-HAND-GESTURE-RECOGNITION-MODEL/blob/main/handgesturess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "datamunge_sign_language_mnist_path = kagglehub.dataset_download('datamunge/sign-language-mnist')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "euT1AWTl6-B9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-11-02T12:01:20.404162Z",
          "iopub.execute_input": "2024-11-02T12:01:20.404685Z",
          "iopub.status.idle": "2024-11-02T12:01:21.784096Z",
          "shell.execute_reply.started": "2024-11-02T12:01:20.404639Z",
          "shell.execute_reply": "2024-11-02T12:01:21.782653Z"
        },
        "trusted": true,
        "id": "5kWYPTaZ6-CQ",
        "outputId": "db3c1aa1-f8f2-419b-d183-aed86902c499"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/sign-language-mnist/sign_mnist_test.csv\n/kaggle/input/sign-language-mnist/amer_sign2.png\n/kaggle/input/sign-language-mnist/amer_sign3.png\n/kaggle/input/sign-language-mnist/sign_mnist_train.csv\n/kaggle/input/sign-language-mnist/american_sign_language.PNG\n/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\n/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\n",
        "\n",
        "# Separate images and labels\n",
        "X_train = train_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1) / 255.0  # Normalize pixel values\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "\n",
        "X_test = test_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1) / 255.0\n",
        "y_test = test_data.iloc[:, 0].values\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=24)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:02:37.853289Z",
          "iopub.execute_input": "2024-11-02T12:02:37.853976Z",
          "iopub.status.idle": "2024-11-02T12:03:00.420436Z",
          "shell.execute_reply.started": "2024-11-02T12:02:37.85393Z",
          "shell.execute_reply": "2024-11-02T12:03:00.418373Z"
        },
        "trusted": true,
        "id": "HAy1k71n6-CV",
        "outputId": "7fd94c01-e951-4113-8e69-5fa560a98d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# One-hot encode labels\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m y_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_test, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/numerical_utils.py:89\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, num_classes))\n\u001b[0;32m---> 89\u001b[0m \u001b[43mcategorical\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     90\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[1;32m     91\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
            "\u001b[0;31mIndexError\u001b[0m: index 24 is out of bounds for axis 1 with size 24"
          ],
          "ename": "IndexError",
          "evalue": "index 24 is out of bounds for axis 1 with size 24",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for unique values in the labels\n",
        "print(\"Unique values in y_train:\", np.unique(y_train))\n",
        "print(\"Unique values in y_test:\", np.unique(y_test))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:04:38.976877Z",
          "iopub.execute_input": "2024-11-02T12:04:38.97749Z",
          "iopub.status.idle": "2024-11-02T12:04:38.989639Z",
          "shell.execute_reply.started": "2024-11-02T12:04:38.977425Z",
          "shell.execute_reply": "2024-11-02T12:04:38.988085Z"
        },
        "trusted": true,
        "id": "Q5W89wGq6-CY",
        "outputId": "99f26c55-9c3b-404f-86a9-2ab2f7dbb2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Unique values in y_train: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\nUnique values in y_test: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remap the labels to make them contiguous\n",
        "y_train = np.where(y_train > 8, y_train - 1, y_train)\n",
        "y_test = np.where(y_test > 8, y_test - 1, y_test)\n",
        "\n",
        "# Verify unique values after remapping\n",
        "print(\"Unique values in y_train after remapping:\", np.unique(y_train))\n",
        "print(\"Unique values in y_test after remapping:\", np.unique(y_test))\n",
        "\n",
        "# One-hot encode labels with num_classes=24\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=24)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:04:44.773241Z",
          "iopub.execute_input": "2024-11-02T12:04:44.774109Z",
          "iopub.status.idle": "2024-11-02T12:04:44.79259Z",
          "shell.execute_reply.started": "2024-11-02T12:04:44.774049Z",
          "shell.execute_reply": "2024-11-02T12:04:44.791038Z"
        },
        "trusted": true,
        "id": "9kS3wc2l6-Ca",
        "outputId": "f7ee34f7-fb60-4aac-c1a3-85759bc09e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Unique values in y_train after remapping: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\nUnique values in y_test after remapping: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(24, activation='softmax')  # 24 classes for A-Y (excluding J)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-02T12:04:50.118951Z",
          "iopub.execute_input": "2024-11-02T12:04:50.119544Z",
          "iopub.status.idle": "2024-11-02T12:07:37.803692Z",
          "shell.execute_reply.started": "2024-11-02T12:04:50.119494Z",
          "shell.execute_reply": "2024-11-02T12:07:37.802381Z"
        },
        "trusted": true,
        "id": "upxiczI26-Cd",
        "outputId": "795bee5d-4d29-42e3-aba2-edd6231a3d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.5033 - loss: 1.7085 - val_accuracy: 0.8699 - val_loss: 0.3961\nEpoch 2/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9891 - loss: 0.0588 - val_accuracy: 0.9052 - val_loss: 0.3420\nEpoch 3/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0119 - val_accuracy: 0.9045 - val_loss: 0.2996\nEpoch 4/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9110 - val_loss: 0.3683\nEpoch 5/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 8.0642e-04 - val_accuracy: 0.9241 - val_loss: 0.3132\nEpoch 6/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 3.9849e-04 - val_accuracy: 0.9229 - val_loss: 0.3336\nEpoch 7/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 2.3871e-04 - val_accuracy: 0.9195 - val_loss: 0.3705\nEpoch 8/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 0.8950 - val_loss: 0.4044\nEpoch 9/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9993 - loss: 0.0050 - val_accuracy: 0.9251 - val_loss: 0.3271\nEpoch 10/10\n\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 2.8516e-04 - val_accuracy: 0.9262 - val_loss: 0.3151\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have X_train (features) and y_train (labels) as your main dataset.\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:07:07.615666Z",
          "iopub.execute_input": "2024-11-01T21:07:07.61615Z",
          "iopub.status.idle": "2024-11-01T21:07:08.441191Z",
          "shell.execute_reply.started": "2024-11-01T21:07:07.616106Z",
          "shell.execute_reply": "2024-11-01T21:07:08.440232Z"
        },
        "trusted": true,
        "id": "IXdWRvZ06-Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load training and validation data\n",
        "train_data = train_datagen.flow_from_directory('path_to_train_data', target_size=(28, 28), color_mode='grayscale', batch_size=32, class_mode='categorical')\n",
        "val_data = val_datagen.flow_from_directory('path_to_val_data', target_size=(28, 28), color_mode='grayscale', batch_size=32, class_mode='categorical')\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout for regularization\n",
        "    Dense(24, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)\n",
        "\n",
        "# Train model with callbacks\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=20,\n",
        "    validation_data=val_data,\n",
        "    callbacks=[early_stopping, lr_reduction]\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(val_data)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}, Validation Loss: {loss:.4f}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:07:30.191718Z",
          "iopub.execute_input": "2024-11-01T21:07:30.192619Z",
          "iopub.status.idle": "2024-11-01T21:07:30.298701Z",
          "shell.execute_reply.started": "2024-11-01T21:07:30.192564Z",
          "shell.execute_reply": "2024-11-01T21:07:30.297147Z"
        },
        "trusted": true,
        "id": "-qVfVOz06-Ch",
        "outputId": "91978868-207c-4011-a2ae-9f6fa3e68cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m val_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load training and validation data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_train_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m val_data \u001b[38;5;241m=\u001b[39m val_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_val_data\u001b[39m\u001b[38;5;124m'\u001b[39m, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_train_data'"
          ],
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_train_data'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:15:11.758523Z",
          "iopub.execute_input": "2024-11-01T21:15:11.75894Z",
          "iopub.status.idle": "2024-11-01T21:15:11.76436Z",
          "shell.execute_reply.started": "2024-11-01T21:15:11.7589Z",
          "shell.execute_reply": "2024-11-01T21:15:11.763157Z"
        },
        "trusted": true,
        "id": "K6nALUdC6-Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Load images and labels from arrays\n",
        "# X should be the images and y should be the corresponding labels\n",
        "# Assuming X is your image data and y is your labels array\n",
        "X = np.load('path_to_images.npy')  # Load your image data\n",
        "y = np.load('path_to_labels.npy')  # Load your labels\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-01T21:15:35.563699Z",
          "iopub.execute_input": "2024-11-01T21:15:35.564121Z",
          "iopub.status.idle": "2024-11-01T21:15:35.613961Z",
          "shell.execute_reply.started": "2024-11-01T21:15:35.564067Z",
          "shell.execute_reply": "2024-11-01T21:15:35.612449Z"
        },
        "trusted": true,
        "id": "UuiVR-7T6-Cm",
        "outputId": "9db5337c-31d5-4659-a9ee-7baa0cdcda57"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Load images and labels from arrays\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# X should be the images and y should be the corresponding labels\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming X is your image data and y is your labels array\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_images.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load your image data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load your labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Split into training and validation sets\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_images.npy'"
          ],
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_images.npy'",
          "output_type": "error"
        }
      ]
    }
  ]
}